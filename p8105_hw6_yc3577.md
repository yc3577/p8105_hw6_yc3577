p8105_hw6_yc3577
================
Yimeng Cai
11/30/2023

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.1     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readr)
library(ggplot2)
library(broom)
library(modelr)
```

    ## 
    ## Attaching package: 'modelr'
    ## 
    ## The following object is masked from 'package:broom':
    ## 
    ##     bootstrap

``` r
library(mgcv)
```

    ## Loading required package: nlme
    ## 
    ## Attaching package: 'nlme'
    ## 
    ## The following object is masked from 'package:dplyr':
    ## 
    ##     collapse
    ## 
    ## This is mgcv 1.9-0. For overview type 'help("mgcv-package")'.

``` r
library(purrr)
library(dplyr)
library(stats)
```

Problem 1

### Problem 1

In the data cleaning code below we create a `city_state` variable,
change `victim_age` to numeric, modifiy victim_race to have categories
white and non-white, with white as the reference category, and create a
`resolution` variable indicating whether the homicide is solved. Lastly,
we filtered out the following cities: Tulsa, AL; Dallas, TX; Phoenix,
AZ; and Kansas City, MO; and we retained only the variables
`city_state`, `resolution`, `victim_age`, `victim_sex`, and
`victim_race`.

``` r
homicide_df = 
  read_csv("homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (8): uid, victim_last, victim_first, victim_race, victim_sex, city, stat...
    ## dbl (4): reported_date, victim_age, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
homicide_df
```

    ## # A tibble: 39,693 × 5
    ##    city_state      resolution victim_age victim_sex victim_race
    ##    <chr>                <dbl>      <dbl> <chr>      <chr>      
    ##  1 Albuquerque, NM          0         15 Female     White      
    ##  2 Albuquerque, NM          0         72 Female     White      
    ##  3 Albuquerque, NM          0         91 Female     White      
    ##  4 Albuquerque, NM          0         56 Male       White      
    ##  5 Albuquerque, NM          0         NA Male       White      
    ##  6 Albuquerque, NM          1         43 Female     White      
    ##  7 Albuquerque, NM          1         52 Male       White      
    ##  8 Albuquerque, NM          1         22 Female     White      
    ##  9 Albuquerque, NM          1         15 Male       Black      
    ## 10 Albuquerque, NM          1         25 Male       Black      
    ## # ℹ 39,683 more rows

Next we fit a logistic regression model using only data from Baltimore,
MD. We model `resolved` as the outcome and `victim_age`, `victim_sex`,
and `victim_race` as predictors. We save the output as `baltimore_glm`
so that we can apply `broom::tidy` to this object and obtain the
estimate and confidence interval of the adjusted odds ratio for solving
homicides comparing non-white victims to white victims.

``` r
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)
baltimore_glm
```

    ## 
    ## Call:  glm(formula = resolution ~ victim_age + victim_sex + victim_race, 
    ##     family = binomial(), data = filter(homicide_df, city_state == 
    ##         "Baltimore, MD"))
    ## 
    ## Coefficients:
    ##      (Intercept)        victim_age    victim_sexMale  victim_raceWhite  
    ##         0.309981         -0.006727         -0.854463          0.841756  
    ## 
    ## Degrees of Freedom: 2752 Total (i.e. Null);  2749 Residual
    ## Null Deviance:       3568 
    ## Residual Deviance: 3493  AIC: 3501

``` r
baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

|    OR | OR_CI_lower | OR_CI_upper |
|------:|------------:|------------:|
| 0.426 |       0.325 |       0.558 |

``` r
baltimore_glm
```

    ## 
    ## Call:  glm(formula = resolution ~ victim_age + victim_sex + victim_race, 
    ##     family = binomial(), data = filter(homicide_df, city_state == 
    ##         "Baltimore, MD"))
    ## 
    ## Coefficients:
    ##      (Intercept)        victim_age    victim_sexMale  victim_raceWhite  
    ##         0.309981         -0.006727         -0.854463          0.841756  
    ## 
    ## Degrees of Freedom: 2752 Total (i.e. Null);  2749 Residual
    ## Null Deviance:       3568 
    ## Residual Deviance: 3493  AIC: 3501

Below, by incorporating `nest()`, `map()`, and `unnest()` into the
preceding Baltimore-specific code, we fit a model for each of the
cities, and extract the adjusted odds ratio (and CI) for solving
homicides comparing non-white victims to white victims. We show the
first 5 rows of the resulting dataframe of model results.

``` r
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

| city_state      |    OR | OR_CI_lower | OR_CI_upper |
|:----------------|------:|------------:|------------:|
| Albuquerque, NM | 1.767 |       0.831 |       3.761 |
| Atlanta, GA     | 1.000 |       0.684 |       1.463 |
| Baltimore, MD   | 0.426 |       0.325 |       0.558 |
| Baton Rouge, LA | 0.381 |       0.209 |       0.695 |
| Birmingham, AL  | 0.870 |       0.574 |       1.318 |

``` r
model_results
```

    ## # A tibble: 47 × 4
    ##    city_state         OR OR_CI_lower OR_CI_upper
    ##    <chr>           <dbl>       <dbl>       <dbl>
    ##  1 Albuquerque, NM 1.77        0.831       3.76 
    ##  2 Atlanta, GA     1.00        0.684       1.46 
    ##  3 Baltimore, MD   0.426       0.325       0.558
    ##  4 Baton Rouge, LA 0.381       0.209       0.695
    ##  5 Birmingham, AL  0.870       0.574       1.32 
    ##  6 Boston, MA      0.667       0.354       1.26 
    ##  7 Buffalo, NY     0.521       0.290       0.935
    ##  8 Charlotte, NC   0.884       0.557       1.40 
    ##  9 Chicago, IL     0.410       0.336       0.501
    ## 10 Cincinnati, OH  0.400       0.236       0.677
    ## # ℹ 37 more rows

Below we generate a plot of the estimated ORs and CIs for each city,
ordered by magnitude of the OR from smallest to largest. From this plot
we see that most cities have odds ratios that are smaller than 1,
suggesting that crimes with male victims have smaller odds of resolution
compared to crimes with female victims after adjusting for victim age
and race. This disparity is strongest in New yrok. In roughly half of
these cities, confidence intervals are narrow and do not contain 1,
suggesting a significant difference in resolution rates by sex after
adjustment for victim age and race.

``` r
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

![](p8105_hw6_yc3577_files/figure-gfm/q1_plot-1.png)<!-- -->

``` r
model_results
```

    ## # A tibble: 47 × 4
    ##    city_state         OR OR_CI_lower OR_CI_upper
    ##    <chr>           <dbl>       <dbl>       <dbl>
    ##  1 Albuquerque, NM 1.77        0.831       3.76 
    ##  2 Atlanta, GA     1.00        0.684       1.46 
    ##  3 Baltimore, MD   0.426       0.325       0.558
    ##  4 Baton Rouge, LA 0.381       0.209       0.695
    ##  5 Birmingham, AL  0.870       0.574       1.32 
    ##  6 Boston, MA      0.667       0.354       1.26 
    ##  7 Buffalo, NY     0.521       0.290       0.935
    ##  8 Charlotte, NC   0.884       0.557       1.40 
    ##  9 Chicago, IL     0.410       0.336       0.501
    ## 10 Cincinnati, OH  0.400       0.236       0.677
    ## # ℹ 37 more rows

Problem 2 For this problem, we’ll use the Central Park weather data
similar to data we’ve seen elsewhere. The code chunk below (adapted from
the course website) will download these data.

First, we need to load in the data.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

    ## using cached file: /Users/christinecym/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2023-11-30 21:48:02.57811 (8.544)

    ## file min/max dates: 1869-01-01 / 2023-11-30

``` r
weather_df
```

    ## # A tibble: 365 × 6
    ##    name           id          date        prcp  tmax  tmin
    ##    <chr>          <chr>       <date>     <dbl> <dbl> <dbl>
    ##  1 CentralPark_NY USW00094728 2022-01-01   201  13.3  10  
    ##  2 CentralPark_NY USW00094728 2022-01-02    10  15     2.8
    ##  3 CentralPark_NY USW00094728 2022-01-03     0   2.8  -5.5
    ##  4 CentralPark_NY USW00094728 2022-01-04     0   1.1  -7.1
    ##  5 CentralPark_NY USW00094728 2022-01-05    58   8.3  -0.5
    ##  6 CentralPark_NY USW00094728 2022-01-06     0   5     1.1
    ##  7 CentralPark_NY USW00094728 2022-01-07    97   1.1  -3.8
    ##  8 CentralPark_NY USW00094728 2022-01-08     0  -1    -6.6
    ##  9 CentralPark_NY USW00094728 2022-01-09    25   4.4  -1.6
    ## 10 CentralPark_NY USW00094728 2022-01-10     0   4.4  -4.3
    ## # ℹ 355 more rows

The boostrap is helpful when you’d like to perform inference for a
parameter / value / summary that doesn’t have an easy-to-write-down
distribution in the usual repeated sampling framework. We’ll focus on a
simple linear regression with tmax as the response with tmin and prcp as
the predictors, and are interested in the distribution of two quantities
estimated from these data: - r̂^ 2 - log(β̂ 1∗β̂ 2) Use 5000 bootstrap
samples and, for each bootstrap sample, produce estimates of these two
quantities.

``` r
bootstrap_sample = function(data) {
  sample_frac(data, replace = TRUE)
}


bootstrap_tibble = 
  tibble(number = 1:5000) |>
  mutate(
    sample = map(number, \(i) bootstrap_sample(weather_df)))

bootstrap_tibble
```

    ## # A tibble: 5,000 × 2
    ##    number sample            
    ##     <int> <list>            
    ##  1      1 <tibble [365 × 6]>
    ##  2      2 <tibble [365 × 6]>
    ##  3      3 <tibble [365 × 6]>
    ##  4      4 <tibble [365 × 6]>
    ##  5      5 <tibble [365 × 6]>
    ##  6      6 <tibble [365 × 6]>
    ##  7      7 <tibble [365 × 6]>
    ##  8      8 <tibble [365 × 6]>
    ##  9      9 <tibble [365 × 6]>
    ## 10     10 <tibble [365 × 6]>
    ## # ℹ 4,990 more rows

``` r
bootstrap_results1 = bootstrap_tibble |>
  mutate(
    models = map(sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy),
    r_squared = map_dbl(models, \(model) summary(model)$r.squared)) |> 
  select(-models) |> 
  unnest(results) 

bootstrap_results1
```

    ## # A tibble: 15,000 × 8
    ##    number sample   term        estimate std.error statistic   p.value r_squared
    ##     <int> <list>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>     <dbl>
    ##  1      1 <tibble> (Intercept)  8.13      0.245       33.2  5.69e-112     0.903
    ##  2      1 <tibble> tmin         1.02      0.0176      57.8  7.64e-185     0.903
    ##  3      1 <tibble> prcp        -0.00363   0.00206     -1.76 7.85e-  2     0.903
    ##  4      2 <tibble> (Intercept)  7.92      0.206       38.4  1.04e-129     0.932
    ##  5      2 <tibble> tmin         1.03      0.0147      70.2  5.52e-213     0.932
    ##  6      2 <tibble> prcp        -0.00594   0.00242     -2.46 1.45e-  2     0.932
    ##  7      3 <tibble> (Intercept)  7.76      0.221       35.2  6.58e-119     0.926
    ##  8      3 <tibble> tmin         1.02      0.0152      67.1  2.22e-206     0.926
    ##  9      3 <tibble> prcp         0.00443   0.00193      2.30 2.21e-  2     0.926
    ## 10      4 <tibble> (Intercept)  7.67      0.208       36.8  2.02e-124     0.932
    ## # ℹ 14,990 more rows

``` r
bootstrap_results2 =
  bootstrap_results1 |>
  group_by(term) |>
  select(number, estimate, r_squared, term) |>
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |>
  mutate(
    log_beta = log(tmin * prcp)
  )
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `log_beta = log(tmin * prcp)`.
    ## Caused by warning in `log()`:
    ## ! NaNs produced

``` r
bootstrap_results2
```

    ## # A tibble: 5,000 × 6
    ##    number r_squared `(Intercept)`  tmin      prcp log_beta
    ##     <int>     <dbl>         <dbl> <dbl>     <dbl>    <dbl>
    ##  1      1     0.903          8.13 1.02  -0.00363    NaN   
    ##  2      2     0.932          7.92 1.03  -0.00594    NaN   
    ##  3      3     0.926          7.76 1.02   0.00443     -5.40
    ##  4      4     0.932          7.67 1.04  -0.00302    NaN   
    ##  5      5     0.909          8.22 1.00  -0.000225   NaN   
    ##  6      6     0.927          7.94 1.02  -0.00300    NaN   
    ##  7      7     0.904          8.24 1.00   0.00630     -5.06
    ##  8      8     0.918          8.34 0.995 -0.00657    NaN   
    ##  9      9     0.931          8.09 1.03  -0.00666    NaN   
    ## 10     10     0.928          8.00 1.04  -0.00615    NaN   
    ## # ℹ 4,990 more rows

Plot the distribution of your estimates, and describe these in words.

``` r
# Then we need to draw the distribution of log_beta

beta_dist =
  bootstrap_results2 |>
  ggplot(aes(x = log_beta)) +
  geom_density(color = 'pink') +
  theme_minimal() +
  labs(
    x = 'Log Beta estimate',
    y = 'Frequency',
    title = 'Distribution of Log beta'
  )

beta_dist
```

    ## Warning: Removed 3435 rows containing non-finite values (`stat_density()`).

![](p8105_hw6_yc3577_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

``` r
hist(bootstrap_results2$log_beta, main = "Distribution of Log beta", xlab = "Log beta", col = "pink")
```

![](p8105_hw6_yc3577_files/figure-gfm/unnamed-chunk-4-2.png)<!-- --> \#
The distribution of Log_beta is above with Pink histogram and pink
density line. It is unimodel. From two graphs, we can see they are left
skewed distributed with a long tail on the left. The range is from -14
to -4 and the center of the mode is around -6. This suggests that there
are a few samples being strapped having a small product of beta
coefficients.

``` r
# Then we need to draw the distribution of r^2

rsquared_dist =
  bootstrap_results2 |>
  ggplot(aes(x = r_squared)) +
  geom_density(color = 'skyblue') +
  theme_minimal() +
  labs(
    x = 'R Squared estimate',
    y = 'Frequency',
    title = 'Distribution of R Squared'
  )

rsquared_dist
```

![](p8105_hw6_yc3577_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

``` r
hist(bootstrap_results2$r_squared, main = "Distribution of R-squared", xlab = "R-squared", col = "skyblue")
```

![](p8105_hw6_yc3577_files/figure-gfm/unnamed-chunk-5-2.png)<!-- --> \#
The distribution of R_squared is above with blue histogram and blue
density curve. It is unimodel. From two graphs, we can see they are also
left skewed a little with a small tail on the left. But the distribution
is closer to normal than the log_beta graph. The range is from 0.86 to
0.95 and the center of the mode is around 0.92.

Using the 5000 bootstrap estimates, identify the 2.5% and 97.5%
quantiles to provide a 95% confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1).
Note: broom::glance() is helpful for extracting r̂ 2 from a fitted
regression, and broom::tidy() (with some additional wrangling) should
help in computing log(β̂ 1∗β̂ 2).

``` r
# Then we can construct the 95% confidence interval for rsquared
rsquared_quantile = 
  bootstrap_results2 |>
  drop_na()|>
  summarize(
    CI_Low = quantile(r_squared, 0.025),
    CI_High = quantile(r_squared, 0.975)
  )
rsquared_quantile
```

    ## # A tibble: 1 × 2
    ##   CI_Low CI_High
    ##    <dbl>   <dbl>
    ## 1  0.883   0.927

# From calculation, the 95% confidence interval for r_squared is (0.8828948, 0.9273118) which means that we have 95 out of 100 times the estimate r_squared will fall between the interval of (0.8828948, 0.9273118).

``` r
# Then we can construct the 95% confidence interval for logbeta
logbeta_quantile = 
  bootstrap_results2 |>
  drop_na()|>
  summarize(
    CI_Low = quantile(log_beta, 0.025),
    CI_High = quantile(log_beta, 0.975)
  )
logbeta_quantile
```

    ## # A tibble: 1 × 2
    ##   CI_Low CI_High
    ##    <dbl>   <dbl>
    ## 1  -8.78   -4.64

# From calculation, the 95% confidence interval for log\_(betatmin \* betaprcp) is (-8.998584, -4.589433) which means that we have 95 out of 100 times the estimate log\_(betatmin \* betaprcp) will fall between the interval of (-8.998584, -4.589433).

Problem 3

In this problem, you will analyze data gathered to understand the
effects of several variables on a child’s birthweight. Load and clean
the data for regression analysis (i.e. convert numeric to factor where
appropriate, check for missing data, etc.).

``` r
birthweight = 
  read_csv('birthweight.csv') |>
  janitor::clean_names() |>
  mutate(
    babysex = recode(babysex, '1' = 'male', '2' = 'female'),
    frace = recode(frace, '1' = 'White', '2' = 'Black', '3' = 'Asian', '4' = 'Puerto Rican', '8' = 'Other', '9' = 'Unknown'),
    mrace = recode(mrace, '1' = 'White', '2' = 'Black', '3' = 'Asian', '4' = 'Puerto Rican', '8' = 'Other'),
    malform = recode(malform, '0' = 'absent', '1' = 'present')
  ) |>
  drop_na()
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
birthweight
```

    ## # A tibble: 4,342 × 20
    ##    babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##    <chr>   <dbl>   <dbl> <dbl> <dbl>   <dbl> <chr>   <dbl> <chr>      <dbl>
    ##  1 female     34      51  3629   177      35 White    39.9 absent        13
    ##  2 male       34      48  3062   156      65 Black    25.9 absent        14
    ##  3 female     36      50  3345   148      85 White    39.9 absent        12
    ##  4 male       34      52  3062   157      55 White    40   absent        14
    ##  5 female     34      52  3374   156       5 White    41.6 absent        13
    ##  6 male       33      52  3374   129      55 White    40.7 absent        12
    ##  7 female     33      46  2523   126      96 Black    40.3 absent        14
    ##  8 female     33      49  2778   140       5 White    37.4 absent        12
    ##  9 male       36      52  3515   146      85 White    40.3 absent        11
    ## 10 male       33      50  3459   169      75 Black    40.7 absent        12
    ## # ℹ 4,332 more rows
    ## # ℹ 10 more variables: mheight <dbl>, momage <dbl>, mrace <chr>, parity <dbl>,
    ## #   pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>, ppwt <dbl>, smoken <dbl>,
    ## #   wtgain <dbl>

Propose a regression model for birthweight. This model may be based on a
hypothesized structure for the factors that underly birthweight, on a
data-driven model-building process, or a combination of the two.

``` r
birthweight_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks +
                          malform + menarche + mheight + momage + mrace + parity + pnumlbw +
                          pnumsga + ppbmi + ppwt + smoken + wtgain, data = birthweight)
birthweight_model
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + 
    ##     frace + gaweeks + malform + menarche + mheight + momage + 
    ##     mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + 
    ##     wtgain, data = birthweight)
    ## 
    ## Coefficients:
    ##       (Intercept)        babysexmale              bhead            blength  
    ##        -6306.8346           -28.7073           130.7781            74.9536  
    ##             delwt            fincome         fraceBlack         fraceOther  
    ##            4.1007             0.2898            -6.9048           -16.9392  
    ## fracePuerto Rican         fraceWhite            gaweeks     malformpresent  
    ##          -68.2323           -21.2361            11.5494             9.7650  
    ##          menarche            mheight             momage         mraceBlack  
    ##           -3.5508             9.7874             0.7593           -60.0488  
    ## mracePuerto Rican         mraceWhite             parity            pnumlbw  
    ##           34.9079            91.3866            95.5411                 NA  
    ##           pnumsga              ppbmi               ppwt             smoken  
    ##                NA             4.3538            -3.4716            -4.8544  
    ##            wtgain  
    ##                NA

Describe your modeling process and show a plot of model residuals
against fitted values – use add_predictions and add_residuals in making
this plot.

``` r
birthweight_df = 
  birthweight |>
  mutate(
    fitted_values = as.vector(fitted(birthweight_model)),
    residuals = as.vector(residuals(birthweight_model)))

birthweight_df
```

    ## # A tibble: 4,342 × 22
    ##    babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##    <chr>   <dbl>   <dbl> <dbl> <dbl>   <dbl> <chr>   <dbl> <chr>      <dbl>
    ##  1 female     34      51  3629   177      35 White    39.9 absent        13
    ##  2 male       34      48  3062   156      65 Black    25.9 absent        14
    ##  3 female     36      50  3345   148      85 White    39.9 absent        12
    ##  4 male       34      52  3062   157      55 White    40   absent        14
    ##  5 female     34      52  3374   156       5 White    41.6 absent        13
    ##  6 male       33      52  3374   129      55 White    40.7 absent        12
    ##  7 female     33      46  2523   126      96 Black    40.3 absent        14
    ##  8 female     33      49  2778   140       5 White    37.4 absent        12
    ##  9 male       36      52  3515   146      85 White    40.3 absent        11
    ## 10 male       33      50  3459   169      75 Black    40.7 absent        12
    ## # ℹ 4,332 more rows
    ## # ℹ 12 more variables: mheight <dbl>, momage <dbl>, mrace <chr>, parity <dbl>,
    ## #   pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>, ppwt <dbl>, smoken <dbl>,
    ## #   wtgain <dbl>, fitted_values <dbl>, residuals <dbl>

``` r
ggplot(birthweight_df, aes(x = fitted_values, y = residuals)) +
  geom_point(aes(color = "Residuals"), alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 1, color = "blue") +
  theme_minimal() +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")
```

    ## `geom_smooth()` using formula = 'y ~ x'

![](p8105_hw6_yc3577_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

Compare your model to two others: One using length at birth and
gestational age as predictors (main effects only)

``` r
# Model with head circumference, length, sex, and all interactions
model1 <- lm(bwt ~ blength + gaweeks, data = birthweight)
model1
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ blength + gaweeks, data = birthweight)
    ## 
    ## Coefficients:
    ## (Intercept)      blength      gaweeks  
    ##    -4347.67       128.56        27.05

One using head circumference, length, sex, and all interactions
(including the three-way interaction) between these

``` r
# Model with head circumference, length, sex, and all interactions
model2 <- lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex, data = birthweight)
model2
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ bhead + blength + babysex + bhead * blength * 
    ##     babysex, data = birthweight)
    ## 
    ## Coefficients:
    ##               (Intercept)                      bhead  
    ##                  -801.949                    -16.598  
    ##                   blength                babysexmale  
    ##                   -21.646                  -6374.868  
    ##             bhead:blength          bhead:babysexmale  
    ##                     3.324                    198.393  
    ##       blength:babysexmale  bhead:blength:babysexmale  
    ##                   123.773                     -3.878

Make this comparison in terms of the cross-validated prediction error;
use crossv_mc and functions in purrr as appropriate.

``` r
set.seed(12)

train_df = sample_n(birthweight, 80)
train_df
```

    ## # A tibble: 80 × 20
    ##    babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##    <chr>   <dbl>   <dbl> <dbl> <dbl>   <dbl> <chr>   <dbl> <chr>      <dbl>
    ##  1 female     34      51  3544   153      96 White    41.9 absent        12
    ##  2 female     31      45  2410   174       5 White    35.7 absent        11
    ##  3 male       34      50  3317   143      55 White    40.7 absent        13
    ##  4 male       36      50  3203   145      15 Black    41.6 absent        12
    ##  5 male       35      52  3289   119      85 White    41.1 absent        11
    ##  6 male       35      50  2438   128      15 Black    37.6 absent        11
    ##  7 female     33      52  3629   136      75 White    36.6 absent        15
    ##  8 female     31      46  2580   119      96 White    38.1 absent        14
    ##  9 male       29      44  1503   127       5 Black    34.7 absent        15
    ## 10 female     34      51  3430   172      45 White    40.9 absent        12
    ## # ℹ 70 more rows
    ## # ℹ 10 more variables: mheight <dbl>, momage <dbl>, mrace <chr>, parity <dbl>,
    ## #   pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>, ppwt <dbl>, smoken <dbl>,
    ## #   wtgain <dbl>

``` r
test_df = anti_join(birthweight, train_df)
```

    ## Joining with `by = join_by(babysex, bhead, blength, bwt, delwt, fincome, frace,
    ## gaweeks, malform, menarche, mheight, momage, mrace, parity, pnumlbw, pnumsga,
    ## ppbmi, ppwt, smoken, wtgain)`

``` r
test_df
```

    ## # A tibble: 4,262 × 20
    ##    babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##    <chr>   <dbl>   <dbl> <dbl> <dbl>   <dbl> <chr>   <dbl> <chr>      <dbl>
    ##  1 female     34      51  3629   177      35 White    39.9 absent        13
    ##  2 male       34      48  3062   156      65 Black    25.9 absent        14
    ##  3 female     36      50  3345   148      85 White    39.9 absent        12
    ##  4 male       34      52  3062   157      55 White    40   absent        14
    ##  5 female     34      52  3374   156       5 White    41.6 absent        13
    ##  6 male       33      52  3374   129      55 White    40.7 absent        12
    ##  7 female     33      46  2523   126      96 Black    40.3 absent        14
    ##  8 female     33      49  2778   140       5 White    37.4 absent        12
    ##  9 male       36      52  3515   146      85 White    40.3 absent        11
    ## 10 male       33      50  3459   169      75 Black    40.7 absent        12
    ## # ℹ 4,252 more rows
    ## # ℹ 10 more variables: mheight <dbl>, momage <dbl>, mrace <chr>, parity <dbl>,
    ## #   pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>, ppwt <dbl>, smoken <dbl>,
    ## #   wtgain <dbl>

``` r
rmse_bw_train = rmse(birthweight_model, train_df)
```

    ## Warning in predict.lm(model, data): prediction from rank-deficient fit; attr(*,
    ## "non-estim") has doubtful cases

``` r
rmse_bw_test = rmse(birthweight_model, test_df)
```

    ## Warning in predict.lm(model, data): prediction from rank-deficient fit; attr(*,
    ## "non-estim") has doubtful cases

``` r
rmse_1_train = rmse(model1, train_df)
rmse_1_test = rmse(model1, test_df)
rmse_2_train = rmse(model2, train_df)
rmse_2_test = rmse(model2, test_df)
```

Note that although we expect your model to be reasonable, model building
itself is not a main idea of the course and we don’t necessarily expect
your model to be “optimal”.
